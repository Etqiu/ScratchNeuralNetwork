{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ed9801e",
   "metadata": {
    "papermill": {
     "duration": 0.003531,
     "end_time": "2025-02-08T23:45:31.796209",
     "exception": false,
     "start_time": "2025-02-08T23:45:31.792678",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Making A Neural Network From Scratch:\n",
    "\n",
    "**Goal**: To implement a neural network using only numPy that has two hidden layers and one output layer with two activation functions. We can then train our neural network on the MNIST data-set, and test it to see our accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c973a3f1",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-02-08T23:45:31.803101Z",
     "iopub.status.busy": "2025-02-08T23:45:31.802858Z",
     "iopub.status.idle": "2025-02-08T23:45:34.123081Z",
     "shell.execute_reply": "2025-02-08T23:45:34.121686Z"
    },
    "papermill": {
     "duration": 2.325471,
     "end_time": "2025-02-08T23:45:34.124602",
     "exception": false,
     "start_time": "2025-02-08T23:45:31.799131",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/mnist-digit-recognizer/train.csv\n",
      "/kaggle/input/chinese-mnist-digit-recognizer/chineseMNIST.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc329bd3",
   "metadata": {
    "papermill": {
     "duration": 0.002818,
     "end_time": "2025-02-08T23:45:34.130822",
     "exception": false,
     "start_time": "2025-02-08T23:45:34.128004",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "I'm going to first import the mnist digits in csv form, every row represents an image and every column, a pixel, with the first column being the label of the number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db355d13",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-08T23:45:34.138037Z",
     "iopub.status.busy": "2025-02-08T23:45:34.137630Z",
     "iopub.status.idle": "2025-02-08T23:45:36.167539Z",
     "shell.execute_reply": "2025-02-08T23:45:36.166293Z"
    },
    "papermill": {
     "duration": 2.035386,
     "end_time": "2025-02-08T23:45:36.169172",
     "exception": false,
     "start_time": "2025-02-08T23:45:34.133786",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>pixel0</th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel774</th>\n",
       "      <th>pixel775</th>\n",
       "      <th>pixel776</th>\n",
       "      <th>pixel777</th>\n",
       "      <th>pixel778</th>\n",
       "      <th>pixel779</th>\n",
       "      <th>pixel780</th>\n",
       "      <th>pixel781</th>\n",
       "      <th>pixel782</th>\n",
       "      <th>pixel783</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  pixel0  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  \\\n",
       "0      1       0       0       0       0       0       0       0       0   \n",
       "1      0       0       0       0       0       0       0       0       0   \n",
       "2      1       0       0       0       0       0       0       0       0   \n",
       "3      4       0       0       0       0       0       0       0       0   \n",
       "4      0       0       0       0       0       0       0       0       0   \n",
       "\n",
       "   pixel8  ...  pixel774  pixel775  pixel776  pixel777  pixel778  pixel779  \\\n",
       "0       0  ...         0         0         0         0         0         0   \n",
       "1       0  ...         0         0         0         0         0         0   \n",
       "2       0  ...         0         0         0         0         0         0   \n",
       "3       0  ...         0         0         0         0         0         0   \n",
       "4       0  ...         0         0         0         0         0         0   \n",
       "\n",
       "   pixel780  pixel781  pixel782  pixel783  \n",
       "0         0         0         0         0  \n",
       "1         0         0         0         0  \n",
       "2         0         0         0         0  \n",
       "3         0         0         0         0  \n",
       "4         0         0         0         0  \n",
       "\n",
       "[5 rows x 785 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"/kaggle/input/mnist-digit-recognizer/train.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f874cbc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-08T23:45:36.178413Z",
     "iopub.status.busy": "2025-02-08T23:45:36.178110Z",
     "iopub.status.idle": "2025-02-08T23:45:36.183350Z",
     "shell.execute_reply": "2025-02-08T23:45:36.182373Z"
    },
    "papermill": {
     "duration": 0.011257,
     "end_time": "2025-02-08T23:45:36.184519",
     "exception": false,
     "start_time": "2025-02-08T23:45:36.173262",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42000, 785)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape # rows show how many pictures there are in the csv "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc11ab5",
   "metadata": {
    "papermill": {
     "duration": 0.003019,
     "end_time": "2025-02-08T23:45:36.191090",
     "exception": false,
     "start_time": "2025-02-08T23:45:36.188071",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "We are going to convert this DataFrame into a np.array, and use test_train_split() by sklearn. Then we want to take the **tranpose** of each array so that each array is actually an individual picture (which will be one of the 784 nodes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "23326ac1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-08T23:45:36.198206Z",
     "iopub.status.busy": "2025-02-08T23:45:36.197937Z",
     "iopub.status.idle": "2025-02-08T23:45:36.475183Z",
     "shell.execute_reply": "2025-02-08T23:45:36.474456Z"
    },
    "papermill": {
     "duration": 0.282148,
     "end_time": "2025-02-08T23:45:36.476306",
     "exception": false,
     "start_time": "2025-02-08T23:45:36.194158",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784,)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "16800"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[2, 9, 3, ..., 9, 0, 9]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[5, 9, 4, ..., 7, 3, 9]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(np.array(df.iloc[:, 1:]), np.array(df[['label']]), test_size = .4)\n",
    "x_train, x_test, y_train, y_test = x_train.T, x_test.T, y_train.T, y_test.T \n",
    "pixels = df.shape[1] - 1\n",
    "display(x_train[:, 0].shape, len(x_test[0]), x_train, y_train, y_test) # check configs\n",
    "# n_xtrain (rows of pixels) = 784, m_xtrain (columns of items) = train/split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f92e5a77",
   "metadata": {
    "papermill": {
     "duration": 0.003201,
     "end_time": "2025-02-08T23:45:36.483167",
     "exception": false,
     "start_time": "2025-02-08T23:45:36.479966",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Above, we see that y_train is actually an array inside an array, we will fix that below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ed0a41d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-08T23:45:36.491106Z",
     "iopub.status.busy": "2025-02-08T23:45:36.490828Z",
     "iopub.status.idle": "2025-02-08T23:45:36.496037Z",
     "shell.execute_reply": "2025-02-08T23:45:36.495109Z"
    },
    "papermill": {
     "duration": 0.010731,
     "end_time": "2025-02-08T23:45:36.497284",
     "exception": false,
     "start_time": "2025-02-08T23:45:36.486553",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([2, 9, 3, ..., 9, 0, 9]), array([5, 9, 4, ..., 7, 3, 9]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train, y_test = y_train[0], y_test[0]\n",
    "y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb788109",
   "metadata": {
    "papermill": {
     "duration": 0.003395,
     "end_time": "2025-02-08T23:45:36.504642",
     "exception": false,
     "start_time": "2025-02-08T23:45:36.501247",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "We also want the RGB values to be scaled from 0-1 so I will divide all training and test X values (the pixels) by 255. Without it some \"gradient explosion\" will happen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb8a2f29",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-08T23:45:36.512918Z",
     "iopub.status.busy": "2025-02-08T23:45:36.512653Z",
     "iopub.status.idle": "2025-02-08T23:45:36.584206Z",
     "shell.execute_reply": "2025-02-08T23:45:36.583016Z"
    },
    "papermill": {
     "duration": 0.077814,
     "end_time": "2025-02-08T23:45:36.585987",
     "exception": false,
     "start_time": "2025-02-08T23:45:36.508173",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "x_train, x_test = x_train / 255, x_test / 255"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbfe38a0",
   "metadata": {
    "papermill": {
     "duration": 0.003606,
     "end_time": "2025-02-08T23:45:36.593435",
     "exception": false,
     "start_time": "2025-02-08T23:45:36.589829",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Note**: Basically we want to make the train set an nxm matrix so we can left multiply it by a weight matrix to move from Rm -> R10 (10 is an arbitrary number for the first layer but makes computation easier). This will help to compute a linear combination with the weights and nodes "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd005fa1",
   "metadata": {
    "papermill": {
     "duration": 0.003411,
     "end_time": "2025-02-08T23:45:36.600470",
     "exception": false,
     "start_time": "2025-02-08T23:45:36.597059",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Initialization and Propagation: \n",
    "1. creating a random array of weights, W1, W2\n",
    "2. creating a random array of biases, b1, b2\n",
    "3. arrays for each layer:\n",
    "    * input layer\n",
    "    * first reLU layer (a function applied to each node)\n",
    "    * second output layer (which will also have a softmax to find a probability distribution of numbers)\n",
    "4. Make forward and backwards propagation. \n",
    "\n",
    "We are also going to start with random weights and biases. Below is a diagram in LaTeX explaining the matrix multiplcation **for a single picture**, where matrix $B$ is our input layer tranposed multiplied by our $A$ matrix, and the biases being added as matrix $C$. The result will be a $10 * m$ matrix, where $m$ is the number of training pictures.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf0ef67",
   "metadata": {
    "papermill": {
     "duration": 0.003281,
     "end_time": "2025-02-08T23:45:36.607415",
     "exception": false,
     "start_time": "2025-02-08T23:45:36.604134",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "\n",
    "\n",
    "$$\\begin{bmatrix}\n",
    "a_{11} & a_{12} & \\cdots & a_{1,784} \\\\\n",
    "a_{21} & a_{22} & \\cdots & a_{2,784} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "a_{10,1} & a_{10,2} & \\cdots & a_{10,784}\n",
    "\\end{bmatrix}\n",
    "\\times\n",
    "\\begin{bmatrix}\n",
    "b_{11} & b_{12} & \\cdots & b_{1m} \\\\\n",
    "b_{21} & b_{22} & \\cdots & b_{2m} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "b_{784,1} & b_{784,2} & \\cdots & b_{784,m}\n",
    "\\end{bmatrix}\n",
    "+\n",
    "\\begin{bmatrix}\n",
    "c_{11} & c_{12} & \\cdots & c_{1m} \\\\\n",
    "c_{21} & c_{22} & \\cdots & c_{2m} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "c_{10,1} & c_{10,2} & \\cdots & c_{10,m}\n",
    "\\end{bmatrix}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf3fe13",
   "metadata": {
    "papermill": {
     "duration": 0.003434,
     "end_time": "2025-02-08T23:45:36.614420",
     "exception": false,
     "start_time": "2025-02-08T23:45:36.610986",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Here are the softmax function and ReLU function definitions:\n",
    "\n",
    "$\n",
    "\\text{softmax}(z_i) = \\frac{e^{z_i}}{\\sum_{j=1}^n e^{z_j}}\n",
    "$\n",
    "\n",
    "where $z_i$ is the i-th element of the input vector z. The softmax function will apply this to each $z_i$.\n",
    "\n",
    "The softmax function is applied independently to each column of $z$. For a single column (logits for one image), softmax computes that equation.\n",
    "\n",
    "$\n",
    "\\text{ReLU}(x) = \\max(0, x)\n",
    "$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1e75fbc3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-08T23:45:36.622584Z",
     "iopub.status.busy": "2025-02-08T23:45:36.622253Z",
     "iopub.status.idle": "2025-02-08T23:45:36.627515Z",
     "shell.execute_reply": "2025-02-08T23:45:36.626676Z"
    },
    "papermill": {
     "duration": 0.0109,
     "end_time": "2025-02-08T23:45:36.628839",
     "exception": false,
     "start_time": "2025-02-08T23:45:36.617939",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def init_parameters(): \n",
    "    # populate a 10 x 784 matrix with random numbers, W1 is weight for first layer\n",
    "    W1 = np.random.rand(10, pixels) -0.5\n",
    "    # continue with biases, which should be a 10x1 vector, which we will broadcast\n",
    "    # 10x1 -> 10xm so we can add the dot product + b\n",
    "    b1 = np.random.rand(10, 1) - 0.5\n",
    "    # second layer weights and biases\n",
    "    W2 = np.random.rand(10, 10) - 0.5\n",
    "    b2 = np.random.rand(10, 1) - 0.5\n",
    "    # we ensure that the weights are centered around 0, instead of 0.5\n",
    "    # we want some negative weights as well so that is why\n",
    "    return W1, b1, W2, b2\n",
    "    \n",
    "def ReLU(Z): # return the same sized matrix, applied reLU\n",
    "    return np.maximum(0,Z)\n",
    "    \n",
    "def SoftMax(Z): # input: 10 x m matrix\n",
    "    # for numerical stability, we can add something to catch nan-values\n",
    "    A = np.exp(Z) / sum(np.exp(Z))\n",
    "    return A\n",
    "\n",
    "    # 1. np.exp(Z) applied to every element in Z \n",
    "    # 2. np.sum(np.exp(Z)) sums all the values of Z from exp, scalar\n",
    "    # 3. All elements are normalized, result = 10 x m, for each training result\n",
    "def forward_propagation(W1, b1, W2, b2, X): # X is the initial input matrix\n",
    "    # Z1 will be the resulting matrix from matrix multiplication\n",
    "    Z1 = W1.dot(X) + b1 \n",
    "    # apply reLU to change from linear combination -> non-linear function\n",
    "    A1 = ReLU(Z1)\n",
    "    Z2 = W2.dot(A1) + b2 # 10 x m\n",
    "    A2 = SoftMax(Z2) # predictions\n",
    "    # return values to update in backpropgation\n",
    "    return Z1, A1, Z2, A2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b41c34c6",
   "metadata": {
    "papermill": {
     "duration": 0.003385,
     "end_time": "2025-02-08T23:45:36.636073",
     "exception": false,
     "start_time": "2025-02-08T23:45:36.632688",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Back Propagation and Updating Weights\n",
    "We will now be writing the backpropagation part: \n",
    "1. We need to write one hot, defined as Y (which is the vector equivalent of what the \"actual\" value is). This will help determine the cost \n",
    "2. Calculate how much each layer contributes to the cost function via partial derivatives:\n",
    "   * dZ is the error of each of the columns (not the cost). This is written as the partial derivative of the cost with respect to the activation (second layer being soft-max and first layer being ReLU).\n",
    "   *  dW is the change in cost with respect to the weights. \n",
    "   *  dB is the change in cost with respect to the biases.<br>\n",
    " For a softmax classifier, we'll use a cross-entropy loss function, which will basically come out to dZ if you take the derivative with respect to activation:\n",
    "\n",
    "$$J(\\hat{y}, y) = -\\sum_{i=0}^{c} y_i \\log(\\hat{y}_i)$$\n",
    "\n",
    "For the entire math behind what dz is, here is a forum: https://community.deeplearning.ai/t/calculating-gradient-of-softmax-function/1897\n",
    "\n",
    "Here are the equations if the neural network was linked by one node at a time. z in this case is the the result of the linear combination of the weights + bias not related to \"activation\" ([3B1B's explanation](https://www.youtube.com/watch?v=Ilg3gGewQ5U&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&index=3))\n",
    ":\n",
    "\n",
    "$\\frac{\\partial C_0}{\\partial w^{(L)}} = \\frac{\\partial z^{(L)}}{\\partial w^{(L)}} \\frac{\\partial a^{(L)}}{\\partial z^{(L)}} \\frac{\\partial C_0}{\\partial a^{(L)}}$\n",
    "\n",
    "$z^{(L)} = w^{(L)} a^{(L-1)} + b^{(L)}$ <br>\n",
    "$a^{(L)} = \\sigma(z^{(L)})$ <br>\n",
    "$C^{(0)} = a^{(L)} - y$\n",
    "\n",
    "Notice how the new layer is based off activation result of the previous layer, this can be also translated into matricies in code you will see below.  \n",
    "\n",
    "By terrible convention, these names dZ, dB, and dW are equal to the following in 3b1b language: \n",
    "\n",
    "$dZ = \\frac{\\partial C_0}{\\partial a^{(L)}}$,\n",
    "$dB = \\frac{\\partial C_0}{\\partial b^{(L)}}$,\n",
    "$dW = \\frac{\\partial C_0}{\\partial w^{(L)}}$,\n",
    "\n",
    "\n",
    "3. Using these equations, we can compute a gradient, which for each layer is a vector that tells us the mangitutde of the effect (cost) of each partial derivative (fancy wording for what we just had above).\n",
    "   \n",
    "4. We will then adjust our weights by subtracting each layer's weights by their specific dW, each layer's biases by their dB. We cannot specifically change the previous layer's (L-1) activation, but used it in helping compute each dW and dB. Here's what it will look like:\n",
    "\n",
    "    $w^{(1)} = w^{(1)} - a * dw^{(1)}$<br>\n",
    "    $b^{(1)} = b^{(1)} - a * db^{(1)}$<br>\n",
    "   $w^{(2)} = w^{(2)} - a * dw^{(2)}$<br>\n",
    "    $b^{(2)} = w^{(2)} - a * db^{(2)}$<br>\n",
    "\n",
    "This is how gradient descent will work, as dw and db will converge to 0 through **minimizing the cost** with respect to all its partial derivitives. Alpha is the learning rate (a)\n",
    "\n",
    "\n",
    "\n",
    "* After all these initialization steps, we will be able to run our own neural network by repitition. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "511f85f3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-08T23:45:36.644081Z",
     "iopub.status.busy": "2025-02-08T23:45:36.643803Z",
     "iopub.status.idle": "2025-02-08T23:45:36.649735Z",
     "shell.execute_reply": "2025-02-08T23:45:36.648741Z"
    },
    "papermill": {
     "duration": 0.011748,
     "end_time": "2025-02-08T23:45:36.651210",
     "exception": false,
     "start_time": "2025-02-08T23:45:36.639462",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def one_hot(Y): # input: the example labels, output: all the one hot Y's\n",
    "    one_hot_Y = np.zeros((Y.size, Y.max() + 1)) \n",
    "    # (#examples, 0-9 *10), we will take transpose later\n",
    "    \n",
    "    one_hot_Y[np.arange(Y.size), Y] = 1\n",
    "    # basically you can put arrays inside index selection in 2d arrays, which is super neat\n",
    "    # this looks like one_hot_Y[(0 - 784 pixels), (4,5,1,2,4... to m images)]\n",
    "    return one_hot_Y.T\n",
    "    \n",
    "def derivative_ReLU(Z): \n",
    "    return Z > 0 # boolean matrix works since True = 1\n",
    "    \n",
    "def back_propagation(Z1, A1, Z2, A2, W2, X, Y): # figuring out partial derivatives\n",
    "    # Z1, Z2 = result of a linear combination of weights + biases\n",
    "    # A1, A2, = result of activation being called on Z1, Z2\n",
    "    # X = training input, Y = ouput results\n",
    "    # reminder that Z1 - A2 are all 10 x m matricies and will broadcast Y.\n",
    "    m, _ = df.shape # number examples\n",
    "    \n",
    "    dZ2 = A2 - one_hot(Y) \n",
    "    # dZ2 = partial d of the change in cost in terms of soft-max activation\n",
    "    # Note this equation is based off of the error because it is the last layer\n",
    "  \n",
    "    dW2 = 1/m * dZ2.dot(A1.T)\n",
    "    # partial d of the second layer costs to weights\n",
    "    # 1/m is applied to every value and is not part of the p derivative\n",
    "    # transpose ensures that matrix multiplication exists\n",
    "   \n",
    "    dB2 = 1/m * np.sum(dZ2, axis = 1, keepdims = True)\n",
    "    # collapses all rows into 10 x m -> 10 x 1, \n",
    "    # partial d of the second layer costs to biases \n",
    "    # also has 1/m applied since this matrix is applied to all training examples\n",
    "    \n",
    "    dZ1 = W2.T.dot(dZ2) * derivative_ReLU(Z1)\n",
    "    # the W2.T activation transpose serves to \"go backwards\" in a sense\n",
    "    # mathmatical proof will for all calculations will be linked below.\n",
    "\n",
    "    dW1 = 1/m * dZ1.dot(X.T)\n",
    "    dB1 = 1/m * np.sum(dZ1, axis = 1, keepdims = True)\n",
    "    # these are actually the same formula as before due to the chain rule !\n",
    "    return dW1, dB1, dW2, dB2 \n",
    "\n",
    "def adjust_parameters(W1, b1, W2, b2, dW1, db1, dW2, db2, alpha): \n",
    "    W1 = W1 - alpha * dW1\n",
    "    b1 = b1 - alpha * db1    \n",
    "    W2 = W2 - alpha * dW2  \n",
    "    b2 = b2 - alpha * db2   \n",
    "    return W1, b1, W2, b2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe7d8294",
   "metadata": {
    "papermill": {
     "duration": 0.003348,
     "end_time": "2025-02-08T23:45:36.658588",
     "exception": false,
     "start_time": "2025-02-08T23:45:36.655240",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Training / Testing the Neural Network\n",
    "We will now be implmeneting the training functionality of the network and test for its accuracy. With gradient descent, the idea is to forward -> backward propagate -> adjust parameters (weights and biases) and then do it again with those new weights and biases. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2ebb0cf1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-08T23:45:36.666885Z",
     "iopub.status.busy": "2025-02-08T23:45:36.666598Z",
     "iopub.status.idle": "2025-02-08T23:46:02.141953Z",
     "shell.execute_reply": "2025-02-08T23:46:02.140989Z"
    },
    "papermill": {
     "duration": 25.481268,
     "end_time": "2025-02-08T23:46:02.143515",
     "exception": false,
     "start_time": "2025-02-08T23:45:36.662247",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 5 4 ... 5 1 8] [2 9 3 ... 9 0 9]\n",
      "Iteration 0 : 0.12563492063492063\n",
      "[1 0 4 ... 0 0 4] [2 9 3 ... 9 0 9]\n",
      "Iteration 20 : 0.18162698412698414\n",
      "[1 0 3 ... 0 0 4] [2 9 3 ... 9 0 9]\n",
      "Iteration 40 : 0.23793650793650795\n",
      "[1 0 3 ... 0 0 3] [2 9 3 ... 9 0 9]\n",
      "Iteration 60 : 0.2811111111111111\n",
      "[1 0 3 ... 0 0 3] [2 9 3 ... 9 0 9]\n",
      "Iteration 80 : 0.31746031746031744\n",
      "[1 0 3 ... 0 0 3] [2 9 3 ... 9 0 9]\n",
      "Iteration 100 : 0.36138888888888887\n",
      "[1 0 3 ... 5 0 3] [2 9 3 ... 9 0 9]\n",
      "Iteration 120 : 0.4083333333333333\n",
      "[1 0 3 ... 5 0 3] [2 9 3 ... 9 0 9]\n",
      "Iteration 140 : 0.45361111111111113\n",
      "[1 9 3 ... 5 0 3] [2 9 3 ... 9 0 9]\n",
      "Iteration 160 : 0.498452380952381\n",
      "[1 9 3 ... 9 0 3] [2 9 3 ... 9 0 9]\n",
      "Iteration 180 : 0.5488095238095239\n",
      "[1 9 3 ... 9 0 3] [2 9 3 ... 9 0 9]\n",
      "Iteration 200 : 0.5962301587301587\n",
      "[1 9 3 ... 9 0 3] [2 9 3 ... 9 0 9]\n",
      "Iteration 220 : 0.6350396825396826\n",
      "[1 9 3 ... 9 0 3] [2 9 3 ... 9 0 9]\n",
      "Iteration 240 : 0.6684126984126985\n",
      "[1 9 3 ... 9 0 3] [2 9 3 ... 9 0 9]\n",
      "Iteration 260 : 0.6918650793650793\n",
      "[1 9 3 ... 9 0 3] [2 9 3 ... 9 0 9]\n",
      "Iteration 280 : 0.7089682539682539\n",
      "[1 9 3 ... 9 0 3] [2 9 3 ... 9 0 9]\n",
      "Iteration 300 : 0.7243650793650793\n",
      "[1 9 3 ... 9 0 3] [2 9 3 ... 9 0 9]\n",
      "Iteration 320 : 0.7361111111111112\n",
      "[1 9 3 ... 9 0 3] [2 9 3 ... 9 0 9]\n",
      "Iteration 340 : 0.7473809523809524\n",
      "[2 9 3 ... 9 0 3] [2 9 3 ... 9 0 9]\n",
      "Iteration 360 : 0.7570238095238095\n",
      "[2 9 3 ... 9 0 3] [2 9 3 ... 9 0 9]\n",
      "Iteration 380 : 0.7664285714285715\n",
      "[2 9 3 ... 9 0 9] [2 9 3 ... 9 0 9]\n",
      "Iteration 400 : 0.7741666666666667\n",
      "[2 9 3 ... 9 0 9] [2 9 3 ... 9 0 9]\n",
      "Iteration 420 : 0.7813888888888889\n",
      "[2 9 3 ... 9 0 9] [2 9 3 ... 9 0 9]\n",
      "Iteration 440 : 0.7879365079365079\n",
      "[2 9 3 ... 9 0 9] [2 9 3 ... 9 0 9]\n",
      "Iteration 460 : 0.7947619047619048\n",
      "[2 9 3 ... 9 0 9] [2 9 3 ... 9 0 9]\n",
      "Iteration 480 : 0.7996825396825397\n"
     ]
    }
   ],
   "source": [
    "def get_predictions(A2):\n",
    "    return np.argmax(A2, axis = 0) \n",
    "    # argmax for a 10 x m array (A2)\n",
    "    # \n",
    "    # second activation layer is the result of SoftMax\n",
    "\n",
    "def accuracy(predictions, Y):\n",
    "    print(predictions, Y)\n",
    "    return np.sum(predictions == Y) / Y.size\n",
    "    # thank you Data 8 for encoding this formula into my head\n",
    "\n",
    "def train_gradient_descent(X, Y, alpha, iterations):\n",
    "    W1, b1, W2, b2 = init_parameters()\n",
    "    #print(\"init: \", W1, b1, W2, b2)\n",
    "    for i in range(iterations): \n",
    "        Z1, A1, Z2, A2 = forward_propagation(W1, b1, W2, b2, X)\n",
    "        #print(\"forward: \", Z1, A1, Z2, A2) *testing\n",
    "        #print(A2.shape)\n",
    "        dW1, db1, dW2, db2  = back_propagation(Z1, A1, Z2, A2, W2, X, Y)\n",
    "        #print(\"back: \", dW1, db1, dW2, db2)\n",
    "        W1, b1, W2, b2 = adjust_parameters(W1, b1, W2, b2, dW1, db1, dW2, db2, alpha)\n",
    "        #print(\"adj: \", W1, b1, W2, b2)\n",
    "        if i % 20 == 0: \n",
    "            predictions = get_predictions(A2)\n",
    "            print(f\"Iteration {i} : {accuracy(predictions, Y)}\")\n",
    "    return W1, b1, W2, b2 \n",
    "\n",
    "W1, b1, W2, b2 = train_gradient_descent(x_train, y_train, 0.1, 500)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e948c48a",
   "metadata": {
    "papermill": {
     "duration": 0.004272,
     "end_time": "2025-02-08T23:46:02.152916",
     "exception": false,
     "start_time": "2025-02-08T23:46:02.148644",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 3147,
     "sourceId": 5196,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 1396483,
     "sourceId": 2314159,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30839,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 33.335067,
   "end_time": "2025-02-08T23:46:02.776585",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-02-08T23:45:29.441518",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
